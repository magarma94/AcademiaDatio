{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df4e982-562f-4ee6-a7e9-4fecda27c496",
   "metadata": {},
   "source": [
    "- #### Arquitectura\n",
    "- #### Flujo de ejecución\n",
    "- #### Transformaciónes vs Acciones\n",
    "- #### Optimizador Catalist\n",
    "- #### DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471db50d-a05e-44c9-8541-af21d996d01b",
   "metadata": {},
   "source": [
    "## Spark architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202feea-0a9a-4f10-a964-a41b52d0b471",
   "metadata": {},
   "source": [
    "![Spark_architecture](../../resources/img/SPARK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da20d3-254a-4467-8fc0-a9d4850bfa05",
   "metadata": {},
   "source": [
    "#### Driver program\n",
    "The process running the main() function of the application and creating the SparkContext. **Starts the application and sends work to the executors.**\n",
    "\n",
    "#### Cluster manager\n",
    "An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, Kubernetes)\n",
    "\n",
    "   - **Standalone** – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    "   - **Apache Mesos** – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "   - **Hadoop YARN** – the resource manager in Hadoop 2 and 3.\n",
    "   - **Kubernetes** – an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "#### Worker node\t\n",
    "Any node that can run application code in the cluster\n",
    "\n",
    "#### Executor\n",
    "A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. **Launched in JVM containers with their own memory and CPU resources**\n",
    "\n",
    "#### Deploy mode\t\n",
    "Distinguishes where the driver process runs. \n",
    "\n",
    "- **Cluster mode** - the Spark driver is launched on a worker node\n",
    "- **Client mode** - the Spark driver is on the client machine\n",
    "- **Local mode** - the entire application runs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490339b3-5622-4c8b-a3be-804726a1762e",
   "metadata": {},
   "source": [
    "## Spark app execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bb527-b442-4b2f-beda-5ed1be6e446b",
   "metadata": {},
   "source": [
    "- An action triggers a **Job**\n",
    "  \n",
    "- A job is split into **stages**\n",
    "    - each stage is dependent on the stage before it\n",
    "    - a stage must fully complete before the next stage can start\n",
    "    - for performance (usually) minimize the number of stages\n",
    "\n",
    "- A stage has **tasks**\n",
    "    - task = smallest unit of work\n",
    "    - tasks are run by executors\n",
    "\n",
    "- **An RDD/DF/DS has partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4e4b79-5329-4734-b919-708b64e069f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.SparkContext@d36df2f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@52207dc\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@d36df2f"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "        .appName(\"sesion_2\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62276910-79b2-48a8-8566-d93c9d126b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgSalaries = MapPartitionsRDD[5] at mapValues at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "employees: org.apache.spark.rdd.RDD[String] = ../../resources/data/csv/employees.csv MapPartitionsRDD[1] at textFile at <console>:28\r\n",
       "empTokens: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "empDetails: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[3] at map at <console>:30\r\n",
       "empGroups: org.apache.spark.rdd.RDD[(String, Iterable[Float])] = ShuffledRDD[4] at groupByKey at <console>:32\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at mapValues at <console>:33"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "// Stage 1\n",
    "val employees: RDD[String] = sc.textFile(\"../../resources/data/csv/employees.csv\")\n",
    "val empTokens: RDD[Array[String]] = employees.map(line => line.split(\",\"))\n",
    "val empDetails: RDD[(String, Float)] = empTokens.map(tokens => (tokens(4), tokens(7).toFloat))\n",
    "// Stage 2\n",
    "val empGroups: RDD[(String, Iterable[Float])] = empDetails.groupByKey(2)\n",
    "val avgSalaries: RDD[(String, Float)] = empGroups.mapValues(salaries => salaries.sum / salaries.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0864d994-8707-4475-b57a-f72681a4928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at mapValues at <console>:33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgSalaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352820d2-4e6f-4bd2-8819-3fd1380de697",
   "metadata": {},
   "source": [
    "#### **TRANSFORMATIONS ARE LAZY - SO ARE EXECUTED WHEN AN ACTION IS TRIGGERED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a307b490-6f59-4adc-ba18-f92ea12a0390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105,67491.2)(103,75527.37)"
     ]
    }
   ],
   "source": [
    "avgSalaries.collect().foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bfde4-d16a-4105-85be-99f3c9efc0b2",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/Stages_tasks.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c791c-6393-473d-9520-dd7e30c42fb8",
   "metadata": {},
   "source": [
    "#### Shuffle\n",
    "\n",
    "- Exchange of data between executors\n",
    "- happens in between stages\n",
    "- must complete before next stage starts\n",
    "\n",
    "- Expensive because of\n",
    "    - transferring data\n",
    "    - serialization/deserialization\n",
    "    - loading new data from shufflefiles\n",
    "- Shuffles are performance bottlenecks because\n",
    "    - exchanging data takes time\n",
    "    - they need to be fully completed before next computations start\n",
    "- Shuffles limit parallelization\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d7fd9-5093-413f-8d0a-c2e2e79f5daf",
   "metadata": {},
   "source": [
    "#### Concepts Relationship\n",
    "App decomposition\n",
    "- 1 job = 1 or more stages\n",
    "- 1 stage = 1 or more tasks\n",
    "\n",
    "Tasks & executors\n",
    "- 1 task is run by 1 executor\n",
    "- each executor can run 0 or more tasks\n",
    "\n",
    "Partitions and tasks\n",
    "- processing 1 partition = one task\n",
    "\n",
    "Partitions & executors\n",
    "- 1 partition stays on 1 executor\n",
    "- each executor can load 0 or more partitions in memory or disk\n",
    "\n",
    "Executors & nodes\n",
    "- 1 executor = 1 JVM on 1 physical node\n",
    "- each physical node can have 0 or more executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead6f94-a013-4f2c-b13f-9fd28b92b6fc",
   "metadata": {},
   "source": [
    "## Transformations vs Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e816447-19b5-49eb-8596-81f48ca6c911",
   "metadata": {},
   "source": [
    "- Transformations describe how new DFs are obtained\n",
    "- Actions start executing Spark code\n",
    "- Transformations return RDDs/DFs\n",
    "- Actions return something else e.g. Unit, number, etc.\n",
    "\n",
    "#### Narrow Transformations\n",
    "\n",
    "- given a parent partition, a single child partition depends on it\n",
    "- fast to compute\n",
    "- examples: union, map, flatMap, select, filter\n",
    "\n",
    "#### Wide transformations\n",
    "\n",
    "- given a parent partition, more than one child partitions depend on it\n",
    "- involve a shuffle = data transfer between Spark executors\n",
    "- are costly to compute\n",
    "- examples: joining, grouping, sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58568431-33d1-4876-bba3-a42832920bac",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d162cb-d21f-410c-a17f-270a50053910",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/Catalist_Optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5d348-df9e-4cc2-87e4-dc609c4ec89f",
   "metadata": {},
   "source": [
    "1. Catalyst Optimizer and Tungsten Execution Engine was introduced in Spark 1.x\n",
    "2. Cost-Based Optimizer was introduced in Spark 2.x\n",
    "3. Adaptive Query Execution now got introduced in Spark 3.x\n",
    "\n",
    "    - To disable the Adaptive Query Execution -> spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "\n",
    "Only works with DF and DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f8997a-da68-4f75-8844-22c9ffcd5a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#8L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=44]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#8L)])\n",
      "         +- Project [id#8L]\n",
      "            +- SortMergeJoin [id#8L], [id#2L], Inner\n",
      "               :- Sort [id#8L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#8L, 200), ENSURE_REQUIREMENTS, [plan_id=36]\n",
      "               :     +- Project [(id#0L * 3) AS id#8L]\n",
      "               :        +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=26]\n",
      "               :           +- Range (1, 100000000, step=1, splits=8)\n",
      "               +- Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=37]\n",
      "                     +- Exchange RoundRobinPartitioning(9), REPARTITION_BY_NUM, [plan_id=29]\n",
      "                        +- Range (1, 100000000, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sum = [sum(id): bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ds1: org.apache.spark.sql.Dataset[Long] = [id: bigint]\r\n",
       "ds2: org.apache.spark.sql.Dataset[Long] = [id: bigint]\r\n",
       "ds3: org.apache.spark.sql.Dataset[Long] = [id: bigint]\r\n",
       "ds4: org.apache.spark.sql.Dataset[Long] = [id: bigint]\r\n",
       "ds5: org.apache.spark.sql.DataFrame = [id: bigint]\r\n",
       "joined: org.apache.spark.sql.DataFrame = [id: bigint]\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[sum(id): bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import java.lang\n",
    "\n",
    "val ds1: Dataset[lang.Long] = spark.range(1, 100000000)\n",
    "val ds2: Dataset[lang.Long] = spark.range(1, 100000000, 2)\n",
    "val ds3: Dataset[lang.Long] = ds1.repartition(7)\n",
    "val ds4: Dataset[lang.Long] = ds2.repartition(9)\n",
    "val ds5: DataFrame = ds3.selectExpr(\"id * 3 as id\")\n",
    "val joined: DataFrame = ds5.join(ds4, \"id\")\n",
    "val sum: DataFrame = joined.selectExpr(\"sum(id)\")\n",
    "sum.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e1e8d-0af3-422f-8e11-83980de000aa",
   "metadata": {},
   "source": [
    "## DAG\n",
    "\n",
    "DAG = Directed Acyclic Graph = visual representation of Spark jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3c540e-2ae2-401b-9ff4-31e9221a95a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(http://23LAP5CD20860NP.indra.es:4041)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ecfda-d392-4e89-a663-0059b42dc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum.count() // Call an action to trigger transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4224e11-d08f-416f-9430-b73e3ea54860",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/DAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9c84b-fce2-41ae-a2f7-0342c7a085f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
