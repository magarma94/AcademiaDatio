{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc26ebe0-9fe4-49ce-9970-c45aa5c280db",
   "metadata": {},
   "source": [
    "#### El cliente ha corroborado que podemos manejar RDD's pero el CORE a utilizar será dataframes, nos han solicitado crear algunas funciones para cargar archivos en formato csv y almacenarlos en formato parquet. ¿Por qué en parquet? Hecha un ojo a esto (vendrá en la evaluación final):\n",
    "#### https://www.databricks.com/glossary/what-is-parquet\n",
    "#### Cuando termines resuelve cada actividad planteada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba45903-1c28-444b-9af0-6442a36468a6",
   "metadata": {},
   "source": [
    "#### Actividad 1:\n",
    "##### TO DO: Crear sparkSession con appName \"ejercicio_4\", y master \"local[*]\", almacenar sesion de spark en una variable llamada \"spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a07f4ae-96d0-48c9-ad50-753c52b0ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaración de librerias\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71db4bec-ce2a-4b44-98d0-4fbdab40ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "# levantar sesion de spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ejercicio_4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d680131-77d5-4429-aa95-24c26c9fda8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://23LAP5CD20860NP.indra.es:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ejercicio_4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22c8c1f5d00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96f213c-5230-4a4d-9b01-d28e073d0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "assert type(spark) == SparkSession\n",
    "assert spark.sparkContext.getConf().get(\"spark.master\") == \"local[*]\"\n",
    "assert spark.sparkContext.getConf().get(\"spark.app.name\") == \"ejercicio_4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c7529-e81d-4bc4-b1b4-abfe69e4b6a5",
   "metadata": {},
   "source": [
    "#### Actividad 2:\n",
    "##### TO DO ->    Crear un método para leer un archivo en formato csv, no inferir el esquema, los archivos si incluyen header, el separador es \",\"\n",
    "- ##### retorna ->  dataframe correspondiente a la lectura\n",
    "- ##### firma ->    def read_csv(path: str) -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "636335bd-d238-410c-ab45-bcdb6c732fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "##def read_csv(path: str) -> DataFrame:\n",
    "    ##return spark.createDataFrame([], StructType([])) # reemplazar por lectura de dataframe\n",
    "\n",
    "def read_csv(path):\n",
    "    return spark.read\\\n",
    "    .option(\"inferSchema\", \"false\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"delimeter\", \",\")\\\n",
    "    .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "099e7f93-2e7e-48f1-941a-2e45da5f0a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cod_client: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- cod_postal: string (nullable = true)\n",
      " |-- vip: string (nullable = true)\n",
      "\n",
      "+----------+-------+----+---------+----------+-----+\n",
      "|cod_client| nombre|edad|provincia|cod_postal|  vip|\n",
      "+----------+-------+----+---------+----------+-----+\n",
      "|     00001|  Julen|  31| Valencia|     23700|false|\n",
      "|     00002| Javier|  58|     Jaén|     08728| true|\n",
      "|     00003| Carlos|  48|  Sevilla|     28757| true|\n",
      "|     00004|  Maria|  38|   Gerona|     18393| true|\n",
      "|     00005|Ernesto|  22|  Cáceres|     13950|false|\n",
      "|     00006|  Luisa|  43|   Murcia|     23940| true|\n",
      "|     00007|   Raul|  51|    Álava|     09030|false|\n",
      "+----------+-------+----+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda validador de prueba:\n",
    "test_path = \"../../resources/data/csv/\"\n",
    "\n",
    "test_clients_df = read_csv(test_path + \"clients.csv\")\n",
    "\n",
    "test_clients_df.printSchema()\n",
    "test_clients_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb49e98-a62e-4324-af0b-626ec21c02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "root_path = \"../../resources/data/csv/\"\n",
    "\n",
    "movies_df = read_csv(root_path + \"movies.csv\")\n",
    "ratings_df = read_csv(root_path + \"ratings.csv\")\n",
    "tags_df = read_csv(root_path + \"tags.csv\")\n",
    "\n",
    "def schema_to_ddl(df):\n",
    "    return spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(df.schema.json()).toDDL().replace(\" NOT NULL\", \"\")\n",
    "\n",
    "assert (type(movies_df) == DataFrame) & (type(ratings_df) == DataFrame) & (type(tags_df) == DataFrame)\n",
    "assert (movies_df.count() == 86537) & (ratings_df.count() == 33832162) & (tags_df.count() == 2328315)\n",
    "assert schema_to_ddl(movies_df) == 'movieId STRING,title STRING,genres STRING'\n",
    "assert schema_to_ddl(ratings_df) == 'userId STRING,movieId STRING,rating STRING,timestamp STRING'\n",
    "assert schema_to_ddl(tags_df) == 'userId STRING,movieId STRING,tag STRING,timestamp STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cb1f3-2308-4ea6-94da-1d61f040941b",
   "metadata": {},
   "source": [
    "#### Actividad 3:\n",
    "##### TO DO ->    Crear un método para escribir un dataframe con las siguientes caracteristicas:\n",
    "- ##### formato: parquet\n",
    "- ##### modo de escritura: overwrite\n",
    "- ##### firma: def write_parquet(df: DataFrame, path: str):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08393772-6be3-4bc8-acea-1856800a3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "#def write_parquet(df: DataFrame, path: str):\n",
    "    #pass # colocar codigo de escritura\n",
    "def write_parquet(DataFrame, path):\n",
    "    DataFrame.write.mode(\"overwrite\").parquet(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d57da1ff-286f-4c9e-aa50-bf3d93ccd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celdas de pruebas:\n",
    "parquet_path = \"../../resources/data/parquet/\"\n",
    "\n",
    "write_parquet(movies_df, parquet_path + \"movies\")\n",
    "write_parquet(ratings_df, parquet_path + \"clients\")\n",
    "write_parquet(tags_df, parquet_path + \"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ee1cea1-a37e-4027-9520-abfca31b3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "\n",
    "root_path = \"../../resources/data/tmp/parquet/04/\"\n",
    "\n",
    "write_parquet(movies_df, root_path + \"movies\")\n",
    "write_parquet(ratings_df, root_path + \"ratings\")\n",
    "write_parquet(tags_df, root_path + \"tags\")\n",
    "\n",
    "_movies_df = spark.read.parquet(root_path + \"movies\")\n",
    "_ratings_df = spark.read.parquet(root_path + \"ratings\")\n",
    "_tags_df = spark.read.parquet(root_path + \"tags\")\n",
    "\n",
    "assert _movies_df.count() == 86537\n",
    "assert _ratings_df.count() == 33832162\n",
    "assert _tags_df.count() == 2328315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d814457-8f21-435f-94cd-ccbc37710670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86537"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cf1ff32-11ba-46a6-864f-8fa59c9bc239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33832162"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "680b99d9-686e-4eba-92fc-040cffc780a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2328315"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712a7e-dbd9-4cdc-ac4c-a1381210a159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
